# ğŸŒ¸ ML Model Deployment using FastAPI & Docker (Iris Dataset)

This project demonstrates how to **train, deploy, and consume a machine learning model** using **FastAPI** ğŸš€ and **Docker** ğŸ³.
The model predicts **Iris flower species** based on input features:

* ğŸŒ± **Sepal Length**
* ğŸŒ± **Sepal Width**
* ğŸŒ¸ **Petal Length**
* ğŸŒ¸ **Petal Width**

---

## ğŸ¯ Goal

* Train a **Random Forest Classifier** to classify iris flower species.
* Save the trained model into a `.joblib` file.
* Expose the model via **REST APIs** built using **FastAPI**.
* Containerize the application with **Docker** for easy deployment.

---

## ğŸ› ï¸ Tools & Technologies

* **Machine Learning:** Random Forest Classifier ğŸŒ³
* **API Framework:** FastAPI âš¡
* **Server:** Uvicorn ğŸ”Œ
* **Containerization:** Docker ğŸ³
* **Dataset:** Iris dataset ğŸŒ¸

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ server.py       # FastAPI app (GET & POST endpoints)
â”‚   â”œâ”€â”€ model.joblib    # Saved ML model
â”‚
â”œâ”€â”€ client.py           # Script to automate predictions
â”œâ”€â”€ requirements.txt    # Required dependencies
â”œâ”€â”€ Dockerfile          # Docker build instructions
â””â”€â”€ README.md           # Project documentation
```

---

## âš™ï¸ Steps to Implement

### 1. ğŸ“Š Train the Model

* Train a **Random Forest Classifier** on the Iris dataset.
* Save the trained model as `model.joblib` using `joblib.dump()`.

---

### 2. ğŸ–¥ï¸ Build the API

* Create `server.py` with FastAPI.
* Load the saved model.
* Implement:

  * âœ… **GET endpoint** â†’ simple health check / welcome message.
  * âœ… **POST endpoint** â†’ accepts flower features & returns predicted species.

---

### 3. ğŸ“¦ Requirements

Create a `requirements.txt` with all dependencies:

```
fastapi
uvicorn
scikit-learn
joblib
```

---

### 4. ğŸ³ Docker Setup

**Dockerfile Overview:**

1. Base image â†’ `FROM python:3.11` ğŸ
2. Create working directory â†’ `WORKDIR /code`
3. Copy and install dependencies â†’ `requirements.txt`
4. Copy app files into container.
5. Expose port `8000` ğŸŒ
6. Run FastAPI app with Uvicorn:

   ```
   CMD ["uvicorn", "app.server:app", "--host", "0.0.0.0", "--port", "8000"]
   ```

---

### 5. ğŸ”¨ Build & Run Docker

* Build image:

  ```bash
  docker build -t iris-fastapi .
  ```
* Run container:

  ```bash
  docker run -p 8000:8000 iris-fastapi
  ```

---

### 6. ğŸŒ Access the API

* Uvicorn runs the server at:
  ğŸ‘‰ [http://127.0.0.1:8000](http://127.0.0.1:8000)

* FastAPI docs available at:
  ğŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

Here you can test the **POST API** by sending flower feature inputs.

**Example Request (POST):**

```json
{
  "features": [5.1, 3.5, 1.4, 0.2]
}
```

**Example Response:**

```json
{
  "prediction": "setosa"
}
```

---

### 7. ğŸ¤– Automating Predictions

Use `client.py` to send multiple flower features in bulk.
The script loops through the input list, sends requests to the API, and prints predictions automatically.

---

## ğŸ§ª Testing the Workflow

1. Run the Docker container.
2. Open [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs).
3. Try out the endpoints:

   * GET â†’ Health check.
   * POST â†’ Send flower features â†’ Get species prediction.
4. Run `client.py` for batch predictions.

---

## ğŸ“Œ Key Notes

* Port `8000` must be exposed for API access.
* Docker ensures the app runs in an **isolated environment**.
* FastAPI auto-generates **interactive API docs** âœ¨.

---

## ğŸš€ Future Enhancements

* Add CI/CD pipeline for deployment.
* Deploy to cloud (AWS/GCP/Azure).
* Extend to handle more datasets & models.

---

### ğŸ’¡ Quick Start

1. Clone repo â†’ `git clone <repo_url>`
2. Build Docker image â†’ `docker build -t iris-fastapi .`
3. Run â†’ `docker run -p 8000:8000 iris-fastapi`
4. Test at â†’ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

ğŸŒ¸ Enjoy predicting Iris flowers with ML + FastAPI + Docker!
